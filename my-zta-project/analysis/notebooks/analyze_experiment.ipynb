{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zero Trust Architecture (ZTA) Experiment Analysis\n",
        "\n",
        "This notebook analyzes the results of ZTA experiments, comparing baseline and ZTA-enabled scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Experiment Data\n",
        "\n",
        "First, we'll load the experiment results from the output directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_experiment(experiment_dir: Path) -> dict:\n",
        "    \"\"\"Load experiment data and return scenario DataFrames.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Load each scenario's events\n",
        "    for scenario_dir in experiment_dir.glob('*'):\n",
        "        if scenario_dir.is_dir():\n",
        "            events_file = scenario_dir / 'events.jsonl'\n",
        "            if events_file.exists():\n",
        "                results[scenario_dir.name] = pd.read_json(\n",
        "                    events_file,\n",
        "                    lines=True,\n",
        "                    convert_dates=['timestamp']\n",
        "                )\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Load experiment data\n",
        "experiment_dir = Path('../data/experiments').glob('*').__next__()\n",
        "scenario_data = load_experiment(experiment_dir)\n",
        "\n",
        "print(f\"Loaded scenarios: {list(scenario_data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Security Metrics Analysis\n",
        "\n",
        "Let's analyze key security metrics across scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_security_metrics(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Calculate security metrics for a scenario.\"\"\"\n",
        "    metrics = {\n",
        "        'total_events': len(df),\n",
        "        'success_rate': df['success'].mean(),\n",
        "        'auth_failures': len(df[~df['success'] & (df['event'] == 'login')]),\n",
        "        'blocked_access': len(df[~df['success'] & (df['event'] == 'access')])\n",
        "    }\n",
        "    \n",
        "    # Attack metrics if present\n",
        "    attack_events = df[df['attack_type'].notna()]\n",
        "    if len(attack_events) > 0:\n",
        "        metrics.update({\n",
        "            'attack_events': len(attack_events),\n",
        "            'attack_success_rate': attack_events['success'].mean()\n",
        "        })\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for each scenario\n",
        "security_metrics = {}\n",
        "for scenario, df in scenario_data.items():\n",
        "    security_metrics[scenario] = calculate_security_metrics(df)\n",
        "\n",
        "# Create comparison plot\n",
        "metrics_df = pd.DataFrame(security_metrics).T\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Security Metrics Comparison')\n",
        "\n",
        "# Success rates\n",
        "metrics_df['success_rate'].plot(kind='bar', ax=axes[0,0], title='Success Rate')\n",
        "axes[0,0].set_ylabel('Rate')\n",
        "\n",
        "# Auth failures\n",
        "metrics_df['auth_failures'].plot(kind='bar', ax=axes[0,1], title='Authentication Failures')\n",
        "axes[0,1].set_ylabel('Count')\n",
        "\n",
        "# Blocked access\n",
        "metrics_df['blocked_access'].plot(kind='bar', ax=axes[1,0], title='Blocked Access Attempts')\n",
        "axes[1,0].set_ylabel('Count')\n",
        "\n",
        "# Attack success rate if present\n",
        "if 'attack_success_rate' in metrics_df.columns:\n",
        "    metrics_df['attack_success_rate'].plot(kind='bar', ax=axes[1,1], title='Attack Success Rate')\n",
        "    axes[1,1].set_ylabel('Rate')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical Analysis\n",
        "\n",
        "Let's perform statistical tests to compare scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_scenarios(baseline_df: pd.DataFrame, zta_df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Perform statistical comparison between scenarios.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Compare success rates\n",
        "    stat, pval = stats.chi2_contingency([\n",
        "        [sum(baseline_df['success']), len(baseline_df) - sum(baseline_df['success'])],\n",
        "        [sum(zta_df['success']), len(zta_df) - sum(zta_df['success'])]\n",
        "    ])[:2]\n",
        "    \n",
        "    results['success_rate_comparison'] = {\n",
        "        'test': 'chi2',\n",
        "        'statistic': stat,\n",
        "        'p_value': pval,\n",
        "        'significant': pval < 0.05\n",
        "    }\n",
        "    \n",
        "    # Compare attack success if present\n",
        "    baseline_attacks = baseline_df[baseline_df['attack_type'].notna()]\n",
        "    zta_attacks = zta_df[zta_df['attack_type'].notna()]\n",
        "    \n",
        "    if len(baseline_attacks) > 0 and len(zta_attacks) > 0:\n",
        "        stat, pval = stats.chi2_contingency([\n",
        "            [sum(baseline_attacks['success']), len(baseline_attacks) - sum(baseline_attacks['success'])],\n",
        "            [sum(zta_attacks['success']), len(zta_attacks) - sum(zta_attacks['success'])]\n",
        "        ])[:2]\n",
        "        \n",
        "        results['attack_success_comparison'] = {\n",
        "            'test': 'chi2',\n",
        "            'statistic': stat,\n",
        "            'p_value': pval,\n",
        "            'significant': pval < 0.05\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Perform comparison if we have baseline and ZTA scenarios\n",
        "if 'baseline' in scenario_data and 'zta_full' in scenario_data:\n",
        "    comparison = compare_scenarios(\n",
        "        scenario_data['baseline'],\n",
        "        scenario_data['zta_full']\n",
        "    )\n",
        "    \n",
        "    print(\"Statistical Comparison Results:\")\n",
        "    for metric, result in comparison.items():\n",
        "        print(f\"\\n{metric}:\")\n",
        "        print(f\"  Test: {result['test']}\")\n",
        "        print(f\"  p-value: {result['p_value']:.4f}\")\n",
        "        print(f\"  Significant: {result['significant']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usability Analysis\n",
        "\n",
        "Now let's analyze the usability metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_usability(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Analyze usability metrics from events.\"\"\"\n",
        "    metrics = {\n",
        "        'avg_task_duration': df['duration'].mean() if 'duration' in df else None,\n",
        "        'friction_events': df['friction_events'].str.len().mean() if 'friction_events' in df else None,\n",
        "        'satisfaction': df['satisfaction_score'].mean() if 'satisfaction_score' in df else None\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate usability metrics\n",
        "usability_metrics = {}\n",
        "for scenario, df in scenario_data.items():\n",
        "    usability_metrics[scenario] = analyze_usability(df)\n",
        "\n",
        "# Plot usability comparison\n",
        "usability_df = pd.DataFrame(usability_metrics).T\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Usability Metrics Comparison')\n",
        "\n",
        "metrics = ['avg_task_duration', 'friction_events', 'satisfaction']\n",
        "titles = ['Average Task Duration', 'Friction Events', 'Satisfaction Score']\n",
        "\n",
        "for ax, metric, title in zip(axes, metrics, titles):\n",
        "    if metric in usability_df.columns:\n",
        "        usability_df[metric].plot(kind='bar', ax=ax, title=title)\n",
        "        ax.set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
